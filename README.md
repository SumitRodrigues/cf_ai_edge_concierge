# Edge Concierge â€” Cloudflare AI (Chat + Memory)

**Stack:**  
- **Frontend:** Vite + React (TypeScript), Tailwind v4, Framer Motion  
- **Backend:** Cloudflare Worker (TypeScript) calling **Workers AI** (Llama 3.3)  
- **State/Memory:** KV (short-term chat history) + Vectorize (long-term snippets)

**Why it stands out:** Minimal, fast, 3D-tinged UI; memory-augmented replies; safe local dev fallback; tool orchestration snippet to demonstrate coordination.

---

## âœ¨ Features
- **LLM on Workers AI:** `@cf/meta/llama-3.3-70b-instruct-fp8-fast`
- **Chat UI:** polished, minimal, smooth scroll & subtle 3D tilt
- **Memory:** KV session history + Vectorize summaries
- **Coordination:** Worker orchestrates memory â†’ optional tool (weather) â†’ LLM  
  *(You can flip in Durable Objects / Workflows later if your account has access.)*

---

## ğŸ§° Prerequisites
- Node 18+
- `npm i -g wrangler` and Cloudflare account
- A workers.dev subdomain (for remote dev & deploy)

---

## âš™ï¸ Local Development

### 1) Backend (Worker)
`
# from repo root
wrangler dev --local
# Ready at http://127.0.0.1:8787
Dev-safe fallback: If Workers AI auth isnâ€™t set up yet, the Worker returns a friendly fallback string so the UI never 500s.`

---

2) Frontend

cd frontend
npm install
npm run dev
# http://localhost:5173
How it connects:

vite.config.ts proxies /api â†’ http://127.0.0.1:8787 in dev, so the UI calls /api/chat.

ğŸ§© Required Cloudflare resources (before deploy)
Create once, then paste IDs into wrangler.toml as needed:

# KV (auto-writes to wrangler.toml if you add --update-config)
wrangler kv namespace create edge-concierge-kv --binding=KV --update-config

# Vectorize (dimensions match @cf/baai/bge-base-en-v1.5)
wrangler vectorize create edge-mem --dimensions=768 --metric=cosine

---

ğŸš€ Deploy

# Backend
wrangler deploy
# Prints https://<name>.<subdomain>.workers.dev

# Frontend
cd frontend
npm run build
# Upload dist/ to Cloudflare Pages (dash) or connect repo to Pages
Production base URL:
If you host the frontend somewhere other than the Worker subdomain, set:

frontend/.env  â†’  VITE_API_BASE=https://<your-worker>.<subdomain>.workers.dev
Rebuild: npm run build.

ğŸ”Œ Optional â€œtoolâ€ (coordination demo)
Add this block in backend/worker.ts before calling the LLM:


ğŸ—ºï¸ Folder map
bash
Copy code
cf_ai_edge_concierge/
â”œâ”€ backend/
â”‚  â””â”€ worker.ts              # Worker: memory + tool + LLM
â”œâ”€ frontend/
â”‚  â”œâ”€ src/App.tsx            # Minimal 3D chat UI
â”‚  â”œâ”€ src/main.tsx
â”‚  â”œâ”€ src/index.css          # Tailwind v4 + custom styles
â”‚  â””â”€ vite.config.ts         # dev proxy
â”œâ”€ wrangler.toml
â”œâ”€ README.md
â””â”€ PROMPTS.md                # prompts you used (see template)
